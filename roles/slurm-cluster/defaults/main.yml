---
# defaults file for slurm-cluster

# ==============================================
# Cluster Configuration
# ==============================================

# Cluster name - used in slurm.conf
slurm_cluster_name: "mycluster"

# Node role: 'controller' or 'compute'
# Set this per-host in your inventory
slurm_node_role: "compute"

# ==============================================
# Controller Node Configuration
# ==============================================

# Controller (slurmctld) hostname - must be resolvable by all nodes
slurm_controller_hostname: "slurm-controller"
slurm_controller_ip: ""

# Backup controller (optional)
slurm_backup_controller_hostname: ""
slurm_backup_controller_ip: ""

# ==============================================
# Compute Node Configuration
# ==============================================

# List of compute nodes with their specifications
# Example:
# slurm_compute_nodes:
#   - name: "compute01"
#     cpus: 4
#     memory: 8192  # MB
#     state: "UNKNOWN"
slurm_compute_nodes:
  - name: "compute01"
    cpus: 4
    memory: 8192
    state: "UNKNOWN"

# Default compute node resources (used if not specified per-node)
slurm_default_cpus: 4
slurm_default_memory: 8192
slurm_default_state: "UNKNOWN"

# ==============================================
# Partition Configuration
# ==============================================

# Default partition name
slurm_partition_name: "batch"

# Partition settings
slurm_partition_default: "YES"
slurm_partition_max_time: "INFINITE"
slurm_partition_state: "UP"

# ==============================================
# Shared Storage Configuration
# ==============================================

# Path to shared NFS storage (required for job data)
slurm_shared_storage_path: "/shared"

# NFS server for shared storage
slurm_nfs_server: ""
slurm_nfs_export: "/export/slurm"

# Mount options for NFS (note: 'intr' is deprecated for NFSv4)
slurm_nfs_mount_options: "rw,sync,hard,nfsvers=4"

# NFS port for availability check
slurm_nfs_port: 2049

# NFS connection timeout (seconds)
slurm_nfs_timeout: 30

# ==============================================
# Slurm Directories
# ==============================================

# Slurm spool directory (must be local on each node)
slurm_spool_dir: "/var/spool/slurm"

# Slurm state directory (should be on shared storage for HA)
slurm_state_dir: "/var/spool/slurmctld"

# Slurm log directory
slurm_log_dir: "/var/log/slurm"

# Slurm PID directory
slurm_pid_dir: "/run/slurm"

# ==============================================
# Authentication
# ==============================================

# Authentication mechanism (munge is standard)
slurm_auth_type: "auth/munge"

# Credential type (cred/none for basic setup, cred/munge for enhanced security)
slurm_cred_type: "cred/none"

# Munge key - should be the same across all nodes
# Generate with: dd if=/dev/urandom bs=1 count=1024 | base64 -w0
# Leave empty to auto-generate on controller and distribute
# IMPORTANT: For production, store this in Ansible Vault
slurm_munge_key: ""

# Munge key size in bytes (for auto-generation)
slurm_munge_key_size: 1024

# Munge directories
slurm_munge_dir: "/etc/munge"
slurm_munge_log_dir: "/var/log/munge"
slurm_munge_run_dir: "/run/munge"

# ==============================================
# Accounting (Optional)
# ==============================================

# Enable Slurm accounting
slurm_accounting_enabled: false

# Accounting storage type
slurm_accounting_storage_type: "accounting_storage/none"

# Database settings (if using slurmdbd)
slurm_db_host: "localhost"
slurm_db_name: "slurm_acct_db"
slurm_db_user: "slurm"
slurm_db_password: ""

# ==============================================
# Scheduler Configuration
# ==============================================

# Scheduler type
slurm_scheduler_type: "sched/backfill"

# Select type (how resources are allocated)
slurm_select_type: "select/cons_tres"
slurm_select_type_parameters: "CR_Core"

# ==============================================
# Resource Limits
# ==============================================

# Default memory per CPU (MB)
slurm_def_mem_per_cpu: 1024

# Maximum memory per CPU (MB) - 0 means unlimited
slurm_max_mem_per_cpu: 0

# Maximum job time (minutes) - 0 means unlimited
slurm_max_time: 0

# ==============================================
# Process Tracking
# ==============================================

# Process tracking type
slurm_proctrack_type: "proctrack/cgroup"

# Task plugin
slurm_task_plugin: "task/affinity,task/cgroup"

# ==============================================
# Logging
# ==============================================

# Log level: quiet, fatal, error, info, verbose, debug, debug2-5
slurm_log_level: "info"

# Slurmctld debug level
slurm_slurmctld_debug: "info"

# Slurmd debug level
slurm_slurmd_debug: "info"

# ==============================================
# Timeouts
# ==============================================

# How often slurmctld checks node state (seconds)
slurm_slurmctld_timeout: 120

# How often slurmd sends heartbeat (seconds)
slurm_slurmd_timeout: 300

# Time before a DOWN node becomes available (seconds)
slurm_return_to_service: 1

# Additional timer settings
slurm_inactive_limit: 0
slurm_kill_wait: 30
slurm_min_job_age: 300
slurm_wait_time: 0

# ==============================================
# Ports
# ==============================================

# Slurmctld port
slurm_slurmctld_port: 6817

# Slurmd port
slurm_slurmd_port: 6818

# ==============================================
# Package Installation
# ==============================================

# Ubuntu 24.04 packages
slurm_common_packages:
  - slurm-wlm
  - slurm-client
  - munge
  - libmunge2

slurm_controller_packages:
  - slurmctld
  - slurm-wlm-doc

slurm_compute_packages:
  - slurmd

# Additional packages for cgroup support
slurm_cgroup_packages:
  - cgroup-tools

# NFS client packages
slurm_nfs_packages:
  - nfs-common

# ==============================================
# Firewall Configuration
# ==============================================

# Enable automatic UFW firewall configuration
# Set to true to automatically open required ports
slurm_configure_firewall: false

# ==============================================
# Service Timeouts
# ==============================================

# Timeout waiting for services to start (seconds)
slurm_service_start_timeout: 60

# Timeout waiting for controller connectivity from compute nodes (seconds)
slurm_controller_connect_timeout: 30

# APT cache validity time (seconds)
slurm_apt_cache_valid_time: 3600

# Package installation retry attempts
slurm_package_install_retries: 3

# Delay between package installation retries (seconds)
slurm_package_install_retry_delay: 5

# ==============================================
# Cgroup Configuration
# ==============================================

# Constrain swap space (recommended for production)
slurm_cgroup_constrain_swap: false

# Memory swappiness (0-100, lower = less swapping)
# Leave empty to use system default
slurm_cgroup_memory_swappiness: ""

# Allowed devices for cgroup constraints
# Set to empty list to use default device list
slurm_cgroup_allowed_devices:
  - "/dev/null"
  - "/dev/zero"
  - "/dev/random"
  - "/dev/urandom"
  - "/dev/tty"
  - "/dev/pts/*"
  - "/dev/ptmx"
  - "/dev/shm/*"
  - "/dev/cpu/*/*"
  - "/dev/cpuset"

# Enable GPU device access (adds /dev/nvidia* and /dev/dri/*)
slurm_cgroup_enable_gpu_devices: false

# ==============================================
# Slurm User Configuration
# ==============================================

# Slurm system user UID (set to ensure consistency across nodes)
# Leave empty for system-assigned UID
slurm_user_uid: ""

# Slurm system group GID
slurm_user_gid: ""
